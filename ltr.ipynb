{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_svmlight_file\n",
    "from pathlib import Path\n",
    "\n",
    "def get_data(file):\n",
    "  return load_svmlight_file(str(file), query_id=True)\n",
    "\n",
    "# could also use the smaller LETOR 4.0 MQ2008 dataset\n",
    "x_train, y_train, qid_train = get_data(Path.home().joinpath('Downloads/MSLR-WEB10K/Fold1/train.txt'))\n",
    "x_test, y_test, qid_test = get_data(Path.home().joinpath('Downloads/MSLR-WEB10K/Fold1/test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def precision(x, at):\n",
    "    num_relevant = len(x['relevance'][lambda y: y > 1])\n",
    "    if num_relevant == 0:\n",
    "        return 1\n",
    "    predicted_relevant = len(x['relevance'][:at][lambda y: y > 1])\n",
    "    return predicted_relevant / min(at, num_relevant)\n",
    "\n",
    "# mean ndcg appears to be slightly different to the xgboost/lightgbm metrics\n",
    "# different input format but same results as https://github.com/lucky7323/nDCG/blob/master/ndcg.py\n",
    "def ndcg(x, at):\n",
    "    if len(x) == 1:\n",
    "        return 1\n",
    "    return ndcg_score(np.asarray([x['relevance']]), np.asarray([x['pred']]), k=at)\n",
    "\n",
    "def evaluate(x, metric='ndcg', at=10):\n",
    "    metrics = {'ndcg': ndcg, 'precision': precision}\n",
    "    if metric not in metrics:\n",
    "        raise ValueError('unsupported metric')\n",
    "    if at < 1:\n",
    "        raise ValueError('k must be >= 1')\n",
    "    return metrics[metric](x, at)\n",
    "\n",
    "df = pd.DataFrame({'qid': qid_test, 'relevance': y_test})\n",
    "(_, uqid_train) = np.unique(qid_train, return_counts=True)\n",
    "(_, uqid_test) = np.unique(qid_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "0.0    124784\n",
       "1.0     77896\n",
       "2.0     32459\n",
       "3.0      4450\n",
       "4.0      1932\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imbalanced dataset, considered all documents with rating > 1 as relevant\n",
    "df.groupby('relevance').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(with_mean=False)),\n",
       "                ('sgdregressor',\n",
       "                 SGDRegressor(loss='epsilon_insensitive', random_state=0))])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a regression model (pointwise)\n",
    "# normalization improves results with linear models, not needed for tree based models\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/sgd.html linear svm using stochastic gradient descent\n",
    "reg = make_pipeline(StandardScaler(with_mean=False), SGDRegressor(loss='epsilon_insensitive', max_iter=1000, tol=1e-3, random_state=0))\n",
    "reg.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler(with_mean=False)),\n",
       "                ('sgdclassifier',\n",
       "                 SGDClassifier(class_weight='balanced', loss='log',\n",
       "                               random_state=0))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a binary logistic regression classification model (pointwise)\n",
    "# Multi-class performs better than single class, but still worse than regression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = make_pipeline(StandardScaler(with_mean=False), SGDClassifier(loss='log', class_weight='balanced', max_iter=1000, random_state=0))\n",
    "clf.fit(x_train, y_train > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRanker(base_score=0.5, booster='gbtree', callbacks=None, colsample_bylevel=1,\n",
       "          colsample_bynode=1, colsample_bytree=0.9, early_stopping_rounds=None,\n",
       "          enable_categorical=False, eval_metric=['ndcg@5', 'ndcg@10'], gamma=0,\n",
       "          gpu_id=0, grow_policy='depthwise', importance_type=None,\n",
       "          interaction_constraints='', learning_rate=0.1, max_bin=256,\n",
       "          max_cat_to_onehot=4, max_delta_step=0, max_depth=8, max_leaves=0,\n",
       "          min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "          n_estimators=200, n_jobs=0, num_parallel_tree=1, predictor='auto',\n",
       "          random_state=0, reg_alpha=0, reg_lambda=1, ...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train xgboost model (LambdaMART pairwise)\n",
    "# could use a grid search to find better parameters\n",
    "from xgboost import XGBRanker\n",
    "\n",
    "xranker = XGBRanker(\n",
    "    tree_method='gpu_hist',\n",
    "    booster='gbtree',\n",
    "    objective='rank:pairwise',\n",
    "    eval_metric=['ndcg@5', 'ndcg@10'],\n",
    "    random_state=0,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=8,\n",
    "    n_estimators=200,\n",
    "    colsample_bytree=0.9,\n",
    "    subsample=0.8,\n",
    ")\n",
    "\n",
    "xranker.fit(\n",
    "    x_train, y_train, group=uqid_train,\n",
    "    eval_set=[(x_test, y_test)],\n",
    "    eval_group=[uqid_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruial\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRanker(colsample_bytree=0.9, num_leaves=127, objective='lambdarank',\n",
       "           random_state=0, subsample=0.8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train lightgbm model (LambdaRank listwise)\n",
    "from lightgbm import LGBMRanker\n",
    "\n",
    "lranker = LGBMRanker(\n",
    "    objective='lambdarank',\n",
    "    random_state=0,\n",
    "    num_leaves=127,\n",
    "    colsample_bytree=0.9,\n",
    "    subsample=0.8,\n",
    ")\n",
    "\n",
    "lranker.fit(\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    group=uqid_train,\n",
    "    eval_set=[(x_test, y_test)],\n",
    "    eval_group=[uqid_test],\n",
    "    eval_at=[5, 10],\n",
    "    eval_metric='ndcg',\n",
    "    verbose=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost 0.48763223098262015\n",
      "lgbm 0.5052920219250704\n"
     ]
    }
   ],
   "source": [
    "print('xgboost', max(xranker.evals_result()['validation_0']['ndcg@10']))\n",
    "print('lgbm', lranker.best_score_['valid_0']['ndcg@10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model._stochastic_gradient.SGDRegressor'>\n",
      "Took 542.1740999999827 ms\n",
      "Mean precision@5 0.42862500000000103\n",
      "Mean precision@10 0.4350412698412699\n",
      "Mean ndcg@5 0.4127612717261256\n",
      "Mean ndcg@10 0.42873475322744653\n",
      "--------\n",
      "<class 'sklearn.linear_model._stochastic_gradient.SGDClassifier'>\n",
      "Took 498.61269999999536 ms\n",
      "Mean precision@5 0.14944166666666597\n",
      "Mean precision@10 0.16473253968253984\n",
      "Mean ndcg@5 0.11674199175018259\n",
      "Mean ndcg@10 0.1413818798710147\n",
      "--------\n",
      "<class 'xgboost.sklearn.XGBRanker'>\n",
      "Took 843.3518999999876 ms\n",
      "Mean precision@5 0.5598833333333332\n",
      "Mean precision@10 0.5373363095238101\n",
      "Mean ndcg@5 0.5310369613409331\n",
      "Mean ndcg@10 0.5370612857860422\n",
      "--------\n",
      "<class 'lightgbm.sklearn.LGBMRanker'>\n",
      "Took 548.4922999999924 ms\n",
      "Mean precision@5 0.5662333333333327\n",
      "Mean precision@10 0.5446097222222225\n",
      "Mean ndcg@5 0.538811934938625\n",
      "Mean ndcg@10 0.5423399822600861\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "# Evaluate different models\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "def evaluate_model(model):\n",
    "    name = model._final_estimator.__class__ if hasattr(model, '_final_estimator') else model.__class__\n",
    "    print(name)\n",
    "    a = timer()\n",
    "    # use probability for the binary classifier model\n",
    "    df['pred'] = model.predict_proba(x_test) if hasattr(model, 'classes_') else model.predict(x_test)\n",
    "    b = timer()\n",
    "    print('Took', (b-a) * 1000, 'ms')\n",
    "    df_sorted = df.sort_values(['qid', 'pred'], ascending=[True, False])\n",
    "    print('Mean precision@5', df_sorted.groupby('qid').apply(evaluate, metric='precision', at=5).mean())\n",
    "    print('Mean precision@10', df_sorted.groupby('qid').apply(evaluate, metric='precision', at=10).mean())\n",
    "    print('Mean ndcg@5', df_sorted.groupby('qid').apply(evaluate, metric='ndcg', at=5).mean())\n",
    "    print('Mean ndcg@10', df_sorted.groupby('qid').apply(evaluate, metric='ndcg', at=10).mean())\n",
    "    print('--------')\n",
    "\n",
    "models = [reg, clf, xranker, lranker]\n",
    "for model in models:\n",
    "    evaluate_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> [ 0.82529851  0.0111614  -0.02176651 ...  0.80119461  0.24418305\n",
      "  0.18839102]\n",
      "-> [ 0.82529851  0.0111614  -0.02176651 ...  0.80119461  0.24418305\n",
      "  0.18839102]\n",
      "-> [[0.55150005]\n",
      " [0.3620873 ]\n",
      " [0.34056926]\n",
      " ...\n",
      " [0.56613636]\n",
      " [0.36450151]\n",
      " [0.18566116]]\n",
      "-> [[0.44849995 0.55150005]\n",
      " [0.6379127  0.3620873 ]\n",
      " [0.65943074 0.34056926]\n",
      " ...\n",
      " [0.43386364 0.56613636]\n",
      " [0.63549849 0.36450151]\n",
      " [0.81433884 0.18566116]]\n",
      "-> [ 0.5361098   0.0264558  -0.33231786 ...  0.95962435 -0.19364479\n",
      " -0.55807936]\n"
     ]
    }
   ],
   "source": [
    "# All models in Scala get the same values\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=False).fit(x_train)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Linear regression\n",
    "print('->', safe_sparse_dot(scaler.transform(x_test), reg._final_estimator.coef_, dense_output=True) + reg._final_estimator.intercept_)\n",
    "print('->', reg.predict(x_test))\n",
    "\n",
    "# Logistic classifier\n",
    "print('->', sigmoid(scaler.transform(x_test) * clf._final_estimator.coef_.T + clf._final_estimator.intercept_))\n",
    "print('->', clf.predict_proba(x_test))\n",
    "\n",
    "# XGBoost ranker\n",
    "print('->', xranker.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could use some model tracking/artifact solution like MLFlow\n",
    "np.savetxt('src/main/resources/scale.csv', [scaler.scale_], delimiter=',')\n",
    "np.savetxt('src/main/resources/reg-coef.csv', [reg._final_estimator.coef_], delimiter=',')\n",
    "np.savetxt('src/main/resources/reg-intercept.csv', reg._final_estimator.intercept_)\n",
    "np.savetxt('src/main/resources/clf-coef.csv', clf._final_estimator.coef_, delimiter=',')\n",
    "np.savetxt('src/main/resources/clf-intercept.csv', clf._final_estimator.intercept_)\n",
    "xranker.save_model('src/main/resources/xgboost.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References\n",
    "# https://www.microsoft.com/en-us/research/project/mslr/\n",
    "# https://github.com/treygrainger/ai-powered-search\n",
    "# https://tech.olx.com/ranking-ads-with-machine-learning-ee03d7734bf4\n",
    "# https://opensourceconnections.com/blog/2017/04/03/test-drive-elasticsearch-learn-to-rank-linear-model/\n",
    "# https://everdark.github.io/k9/notebooks/ml/learning_to_rank/learning_to_rank.html\n",
    "# https://www.elastic.co/blog/introducing-approximate-nearest-neighbor-search-in-elasticsearch-8-0\n",
    "# https://haystackconf.com/us2021/talk-6/\n",
    "# https://arxiv.org/pdf/1803.05127.pdf\n",
    "# https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/letor3.pdf\n",
    "# https://towardsdatascience.com/learning-to-rank-a-complete-guide-to-ranking-using-machine-learning-4c9688d370d4\n",
    "\n",
    "# Building good training data sets for ranking is hard. Non ML techniques are frequently used in information retrieval.\n",
    "# For search engines, we can rank by summing the tf-idf of each query term or apply the more advanced Okapi BM25 ranking function.\n",
    "# To compare documents, it is common to use the cosine similarity (~= dot product of normalized TF-IDF vectors or document embeddings).\n",
    "# Document embeddings are computed with doc2vec or by averaging word/sentence embeddings (Transformers like Universal Sentence Encoder are popular).\n",
    "# Word embeddings can be computed using a PMI matrix + SVD or sophisticated ML based algorithms like GloVe and word2vec."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58873ede596a2747dd7f16e364cae25e7acc47924c64b83e83112f7e389e5939"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
